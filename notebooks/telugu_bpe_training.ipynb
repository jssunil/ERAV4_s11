{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Telugu BPE Training and Evaluation\n",
        "\n",
        "This notebook builds a Byte-Pair Encoding (BPE) tokenizer for Telugu text collected from Wikipedia, saves the learned vocabulary, and evaluates its compression ratio on the training corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import math\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Iterable, List, Sequence, Tuple\n",
        "\n",
        "CORPUS_PATH = Path(\"../data/telugu_corpus.txt\").resolve()\n",
        "MERGES_PATH = Path(\"../data/telugu_bpe_merges.txt\").resolve()\n",
        "VOCAB_PATH = Path(\"../data/telugu_bpe_vocab.txt\").resolve()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_text = CORPUS_PATH.read_text(encoding=\"utf-8\")\n",
        "print(f\"Corpus characters: {len(raw_text):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_VOCAB_SIZE = 6000\n",
        "MAX_MERGES = 12000\n",
        "\n",
        "Pair = Tuple[str, str]\n",
        "Word = Tuple[str, ...]\n",
        "\n",
        "\n",
        "def clean_and_tokenize(text: str) -> List[str]:\n",
        "    filtered = re.sub(r\"[^\\u0C00-\\u0C7F\\s]\", \" \", text)\n",
        "    normalized = re.sub(r\"\\s+\", \" \", filtered).strip()\n",
        "    if not normalized:\n",
        "        return []\n",
        "    return normalized.split()\n",
        "\n",
        "\n",
        "def build_initial_vocab(words: Iterable[str]) -> Counter[Word]:\n",
        "    vocab: Counter[Word] = Counter()\n",
        "    for word, freq in Counter(words).items():\n",
        "        symbols = tuple(list(word) + [\"</w>\"])\n",
        "        vocab[symbols] = freq\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def get_pair_stats(vocab: Counter[Word]) -> Dict[Pair, int]:\n",
        "    stats: Dict[Pair, int] = defaultdict(int)\n",
        "    for symbols, freq in vocab.items():\n",
        "        for idx in range(len(symbols) - 1):\n",
        "            stats[(symbols[idx], symbols[idx + 1])] += freq\n",
        "    return stats\n",
        "\n",
        "\n",
        "def merge_pair(pair: Pair, word: Word) -> Word:\n",
        "    merged: List[str] = []\n",
        "    i = 0\n",
        "    while i < len(word):\n",
        "        if i < len(word) - 1 and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
        "            merged.append(word[i] + word[i + 1])\n",
        "            i += 2\n",
        "        else:\n",
        "            merged.append(word[i])\n",
        "            i += 1\n",
        "    return tuple(merged)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BPEModel:\n",
        "    merges: List[Pair]\n",
        "    vocab: List[str]\n",
        "\n",
        "    def encode_word(self, word: str) -> List[str]:\n",
        "        symbols: List[str] = list(word) + [\"</w>\"]\n",
        "        if not symbols:\n",
        "            return []\n",
        "\n",
        "        merge_ranks = {pair: idx for idx, pair in enumerate(self.merges)}\n",
        "\n",
        "        def get_pairs(sequence: Sequence[str]) -> set[Pair]:\n",
        "            return {(sequence[i], sequence[i + 1]) for i in range(len(sequence) - 1)}\n",
        "\n",
        "        while True:\n",
        "            pairs = get_pairs(symbols)\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            ranked_pairs = {pair: merge_ranks.get(pair, math.inf) for pair in pairs}\n",
        "            best_pair = min(ranked_pairs, key=ranked_pairs.get)\n",
        "            if ranked_pairs[best_pair] is math.inf:\n",
        "                break\n",
        "\n",
        "            new_symbols: List[str] = []\n",
        "            i = 0\n",
        "            while i < len(symbols):\n",
        "                if (\n",
        "                    i < len(symbols) - 1\n",
        "                    and symbols[i] == best_pair[0]\n",
        "                    and symbols[i + 1] == best_pair[1]\n",
        "                ):\n",
        "                    new_symbols.append(symbols[i] + symbols[i + 1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_symbols.append(symbols[i])\n",
        "                    i += 1\n",
        "            symbols = new_symbols\n",
        "\n",
        "        if symbols and symbols[-1] == \"</w>\":\n",
        "            symbols = symbols[:-1]\n",
        "        return symbols\n",
        "\n",
        "\n",
        "def learn_bpe(vocab: Counter[Word]) -> BPEModel:\n",
        "    merges: List[Pair] = []\n",
        "    symbol_set = {symbol for word in vocab for symbol in word}\n",
        "\n",
        "    for iteration in range(MAX_MERGES):\n",
        "        if len(symbol_set) >= TARGET_VOCAB_SIZE:\n",
        "            break\n",
        "\n",
        "        pair_stats = get_pair_stats(vocab)\n",
        "        if not pair_stats:\n",
        "            break\n",
        "\n",
        "        best_pair, frequency = max(pair_stats.items(), key=lambda item: item[1])\n",
        "        if frequency < 2 and len(symbol_set) >= TARGET_VOCAB_SIZE:\n",
        "            break\n",
        "\n",
        "        vocab = Counter({merge_pair(best_pair, word): freq for word, freq in vocab.items()})\n",
        "        merges.append(best_pair)\n",
        "        symbol_set.add(best_pair[0] + best_pair[1])\n",
        "\n",
        "        if (iteration + 1) % 200 == 0:\n",
        "            print(\n",
        "                f\"[Iteration {iteration + 1}] symbols={len(symbol_set)} last_pair={best_pair}\"\n",
        "            )\n",
        "\n",
        "    print(f\"Finished training after {len(merges)} merges; vocab size={len(symbol_set)}.\")\n",
        "    return BPEModel(merges=merges, vocab=sorted(symbol_set))\n",
        "\n",
        "\n",
        "def evaluate_bpe(model: BPEModel, words: Sequence[str]) -> Tuple[int, int, float]:\n",
        "    total_characters = sum(len(word) for word in words)\n",
        "    encoded_token_count = 0\n",
        "\n",
        "    for word in words:\n",
        "        encoded_token_count += len(model.encode_word(word))\n",
        "\n",
        "    if encoded_token_count == 0:\n",
        "        raise RuntimeError(\"Encoding produced zero tokens.\")\n",
        "\n",
        "    compression_ratio = total_characters / encoded_token_count\n",
        "    return total_characters, encoded_token_count, compression_ratio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "words = clean_and_tokenize(raw_text)\n",
        "print(f\"Unique words: {len(set(words)):,}; Total word instances: {len(words):,}\")\n",
        "\n",
        "initial_vocab = build_initial_vocab(words)\n",
        "print(f\"Initial symbol count: {len({symbol for word in initial_vocab for symbol in word}):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bpe_model = learn_bpe(initial_vocab)\n",
        "print(f\"Learned merges: {len(bpe_model.merges):,}\")\n",
        "print(f\"Learned vocabulary size (including </w>): {len(bpe_model.vocab):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_chars, total_tokens, compression_ratio = evaluate_bpe(bpe_model, words)\n",
        "print(f\"Characters encoded: {total_chars:,}\")\n",
        "print(f\"Tokens produced: {total_tokens:,}\")\n",
        "print(f\"Compression ratio (chars per token): {compression_ratio:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MERGES_PATH.write_text(\"\\n\".join(f\"{l} {r}\" for l, r in bpe_model.merges), encoding=\"utf-8\")\n",
        "VOCAB_PATH.write_text(\"\\n\".join(bpe_model.vocab), encoding=\"utf-8\")\n",
        "print(f\"Saved merges to {MERGES_PATH}\")\n",
        "print(f\"Saved vocab to {VOCAB_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "- Vocabulary size exceeds the 5,000-token requirement.\n",
        "- Compression ratio (characters per token) is above 3, satisfying the target constraint.\n",
        "- The merge operations and vocabulary are saved under `data/` for reuse.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.13 (with PyTorch)",
      "language": "python",
      "name": "python313-torch"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
